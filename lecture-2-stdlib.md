# Lecture 2：标准库

程序设计语言给开发者提供了最基础的手段，可以用它编一些程序了。但是现代软件项目的规模都非常大，动辄数十万行代码，涉及到I/O、网络、数据库、并发等方方面面的知识和技能。就好比生产一辆汽车，如果所有的部件都要自己造的话，难度无疑非常大。这个“难度”包括两个方面：其一是技能，并非每个人、每个团队都具备从底层到应用的技术能力；其二是成本，即便聚齐了一个梦之队，但猴子买成马价钱，也太不划算。所以，软件开发是一个很特殊的行业，开发者之间会相互依赖，从而形成一个彼此依存的复杂的生态环境，我们称之为开发者社区（Developer Community）。尤其有趣的是，软件开发可能是雷锋最多的行业了，大量的开发者不辞辛劳地创造一些公共资源（包括工具/Utilities，框架/Frameworks，组件/Components、库/Libraries），让其他素不相识的开发者、开发组织在遵循一定许可（License）的前提下免费使用。这是一种了不起的奉献精神，也是程序员这个职业最让人引以为自豪的原因之一。尽管这个世界充满了利益纠缠，从满了尔虞我诈，充满了铜臭味，但开发者之间的这种特殊的依存关系却是基于信任而运作的：您不会怀疑OpenSSL或者Log4j会恶意删掉您计算机里的重要文件，也不会怀疑它们会向您的客户发送骚扰信息，尽管它们的代码里可能存在各种各样的Bug。一个合格的程序员（Programmer）应该具备一些优秀的品质，比如：

1. 懒惰。如果缺某个东西，或者自己不会做某个东西，就先假设它已经有了，一定有人已经做了并且能为我所用；
2. 信任。应该合理信任其他开发者所做的成果，同时也要努力让自己的工作对他人而言是可信任的（意图和质量）；
3. 创新。保持一种不满于现状的、善意开放的心态，对于某个东西，如果有更好的方案，那就大胆地重新造个轮子。

Java之所以能够成为一个举足轻重的软件开发平台，和它有一个活跃的的社区关系很密切。像Apache软件基金会（Apache Software Foundation）、Eclipse基金会（Eclipse Foundation）这样的开源社区，贡献了大量的高质量、基础性的软件项目，构成了Java 生态的核心，几乎在任何一个用Java开发的软件项目里都能找到它们的存在。这一类依赖，统称为第三方（Third Party）。而开发者自己写的代码，当然就是“第二方”了，第一方是谁呢？那就是JDK为开发者所提供的便利，称之为标准库（Standard Libraries）。如果说语言是第一等公民的话，标准库就是第二等公民，它封装/实现了基础的、公共的操作，或者弥补语言本身所欠缺的特性。一般来讲，标准库所提供的实现，都是趋于最优的，是值得信赖的。任何一种工业级的语言，都有实现自己的标准库，否则就是一个没什么用处的玩具。

Java标准库（也称为基础类库）所提供的都是引用类型，以java.lang.Object为根，它们被组织成一个层次严谨的树状结构。这一讲，将深入地讨论Java标准库中的若干重要内容，但不能代替[标准库的API手册](https://docs.oracle.com/en/java/javase/17/docs/api/index.html)。

## 2.1 字符串和文本

字符串（String）是一个使用频率非常高的引用类型，可以把它看作是一个由基本类型字符（char）构成的数组。在JVM内部，采用Unicode（UTF-16）作为字符集编码，对于非ISO-8859-1字符集的字符（比如中文的GB2312/GBK编码），开发者要注意不同编码之间的转换，否则就会出现一些没意义的乱码。

### 2.1.1 编码简史

世界上现存的语言文字有5000多种，有些是拉丁字母组合，有些是象形符号组合（比如中日韩的文字，CJK），有些从右往左读写（阿拉伯文）......千奇百怪，计算机怎么才能录入和显示它们？一种最简单的想法就是：给每一种语言的每一个基本构成部分编一个号，将文字做成字库，输入这个数字它就显示对应的符号。

事实上计算机科学家们也是这么干的。计算机最先是在欧美出现的，它们就把英文字母以及一些常用的符号编了码，形成了最早的字符集：ASCII（American Standard Code for Information Interchange，美国信息交换标准代码）。它包含四部分：

- 非显示的控制字符，比如回车、控制符等，编码范围是从0 - 31，以及127，共33个；
- 阿拉伯数字，就是0 - 9，编码范围从48 - 57， 共10个；
- 大小写拉丁字母，A - Z， a - z，编码范围从65 - 122，共52个；
- 可见的非控制字符，比如空格符，%，$，*，；，+等等，编码范围有三个区域：
  - 从32 - 47，共16个；
  - 从91 - 96，共6个；
  - 从123 - 126，共4个。

从编码分布可以看出一个有趣现象：即便是最小的ASCII字符集，也是在不断扩展和完善。开始没考虑全面，后面又补充一些进来，按正常人的思维，应该将可见非显示字符放在一起，而不是分成几个部分。确实如此，ASCII标准第一版发布的时间是1967年，而最后一次修订的时间是1986年，也被国际标准化组织（ISO）纳为国际标准了，编号是ISO/IEC 646。

ASCII字符集的容量，就是一个byte所能表示的非负数范围，即0 - 127，共128个字符，处理由英文构成的文字是没问题了。但是像中国这样的国家怎么办呢，就非得把汉字拉丁化不可吗？随之而来的就是各种各样的字符集编码出现了。首先是一些西欧国家，文字里有一些奇怪的符号的比如[œ](https://baike.baidu.com/item/œ)、[Œ](https://baike.baidu.com/item/Œ)、[Ÿ](https://baike.baidu.com/item/Ÿ) 这样的东西，ASCII码里就不包含。还有泰语、希伯来语等，ASCII都不支持。于是就需要另起炉灶，创造一种新的字符集编码方案。计算机世界里有个很有趣的概念叫做“向下兼容”（Backwards Compatibility），后来者可以搞新花样，但是大多数情况下，不能推翻或否定之前的已成为事实的东西，比如：64位的Windows上，要能运行32位的应用程序；新设计的API要顾及老版本API的使用者，至少签名要一致，等等。向下兼容性会成为软件开发的一个沉重的包袱，但又无法甩掉。

字符集扩展也如此。大家新搞了个ISO/IEC8859的编码体系，但是必须要兼容ASCII编码。ASCII编码利用了一个byte的7个bit（2^7），ISO/IEC 8859的第一个标准8859-1（西欧语言，俗称Latin-1）扩展了一位，相当于一个unsigned byte的表示范围，就可以容纳256个字符了，其分布如下：

- 128 - 159，未定义的范围。有些系统在这个范围内自定义了一些字符，比如Windows-1252编码；
- 160 - 191，一些常用字符，包括人民币的￥符号，版权、注册商标等符号；
- 192 - 255，被称为高段编码，主要是西欧字符。

ISO/IEC 8859是一个系列规范，只不过Latin-1更常用。我们回到中文世界看看，汉字编码的历程。1980年，国家标准总局发布了GB2312（信息交换用汉字编码字符集.基本集，又称作GB2312-1980, GB0），成为简体中文的编码标准，并于当年5月1日实施。GB2312字符集共收录了6000多个汉字，还包含了拉丁字母、平假名/片假名、俄语字母等，除了繁体字、古汉语的一些生僻字之外，基本能够满足计算机中文处理的需要了。

GB2312是双字节编码，它采用了一种叫做“区位码”的编码方案，先将编码空间分为94个区，每个区可容纳94个汉字/符号，构成一个94*94的矩阵，最多可以容纳8836个字符。它用第一个字节表示区，第二个字节表示位，就跟士兵列队一样，其分布如下：

- 1 - 9区，包括了Latin-1以及俄语字母、日文平假名/片假名，特殊字符等；
- 10 -15区，未定义的保留区，扩展使用；
- 16 - 55区，常用汉字，也叫一级汉字，按拼音字母排序；
- 56 - 87区，非常用汉字，也叫二级汉字，按偏旁部首排序；
- 88 - 94区，未定义的保留区，扩展使用。

一个byte可以表示的非负范围是0 - 127，那么为什么GB2312中，同样是用一个byte表示的区和位最大只有94？两个原因：

- 避开ASCII编码中的控制符（0 - 31，127）和空格符（32）；
- 区位的编码从1开始，而不是从0开始，这点有违程序员的直觉。

那么，128 - 33 - 1 = 94了。

同样是双字节，容量却比2^15 = 32768 小了很多。但是区位码方案的好处在于可以分组，便于检索定位。假设有这么一个需求：为了简化输入，经常会用到汉字的拼音首字母作为缩写，例如：郭靖 -> GJ，计算机 -> JSJ，世界人民大团结万岁 - > SJRMDTJWS，除了导入一个字典之外，还有什么简便的方法可以实现呢？请看这个class，利用了区位码里，常用汉字按拼音字母排序的规则：

```java
public class Pinyin {
	private final static int[] SP_BOUNDARIES = { 1601, 1637, 1833, 2078, 2274,
			2302, 2433, 2594, 2787, 3106, 3212, 3472, 3635, 3722, 3730, 3858,
			4027, 4086, 4390, 4558, 4684, 4925, 5249, 5590 };
	private final static String[] FIRST_LETTERS = { "A", "B", "C", "D", "E",
			"F", "G", "H", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S",
			"T", "W", "X", "Y", "Z" };

	public static String getFirstLetters(String chinese) {
		if (chinese == null) return "";
		var result = new StringBuilder();
		char[] chars = chinese.toCharArray();
		for(char ch : chars) {
			result.append(firstLetters(String.valueOf(ch)));
		}
		return result.toString();
	}

 	private static String firstLetters(String chinese) {
		var result = convertCharset(chinese, "GB2312", "ISO8859-1");
		if (result.length() > 1) { //It's Chinese chars
			int sectorCode = (int)result.charAt(0) - 160; //Section Code
			int positionCode = (int)result.charAt(1) - 160; //Position Code
			int secPosCode = sectorCode * 100 + positionCode; //Sec-Pos Code
			if (secPosCode > 1600 && secPosCode < 5590) {
				for (int i = 0; i < 23; i++) {
					if(secPosCode < SP_BOUNDARIES[i]) continue;
					if(secPosCode >= SP_BOUNDARIES[i + 1]) continue;
					result = FIRST_LETTERS[i]; break;
				}
			}else{
				result = convertCharset(chinese, "ISO8859-1", "GB2312");
				result = "".equals(result) ? "" : result.substring(0, 1);
			}
		}
		return result;
	}
 	
	private static String convertCharset(String source, String sourceCharset,String destCharset) {
		try {
			return new String(source.getBytes(sourceCharset), destCharset);
		} catch (UnsupportedEncodingException ex) { return "";}
	}
}
```

请注意，GB2312和ASCII码是不兼容的。即便同样的符号，GB2312采用了16位重新编码之后，屏幕显示时占双倍像素宽度，就是我们通常所说的“全角字符”（Full-width），而在ASCII码里占的标准像素宽度的叫做“半角字符”（Half-Width）。在中文环境下编程偶尔会出这样的错误，比如分号、括号等，输入了全角字符。如果听信一些所谓的“编程高手”的话，用文本编辑器而不是现代的IDE，一个简单的错误要看瞎眼睛。

上文说到，为了避开ASCII控制字符，GB2312的编码空间只剩下94个了，只有33 - 126这个区间，是可用的。所以GB2312规定，编码高低字节都是从33（0X21）开始，至126（0xEE）结束，也就是说，把前述的区位码编码方案，都加上32（0x20）即可。我们把这个遵循[33, 126]规则的汉字区位编码，称之为“国标码”。

不兼容始终是一个问题，因为全世界用的都是相同的计算机，总不能让中文世界永远不与西方进行信息交换。所以势必还是要想办法解决这个问题。人类善于修城墙搞围堵，也擅长挖运河搞疏通，既然ASCII码最高是127，那就把国标码往后面移动128个空间（相当于把字节的最高位设置为1），也就是说，国标码的最低编码空间是161了，就不会覆盖ASCII码的[33, 126]这个区间了，换言之，国标码实现了与ASCII码的兼容，我们把这种编码规则称之为“机内码”（也叫作EUC-CN），它是一种汉字编码的存储方案，需要与国标码之间进行转换。区位码、国标码、机内码之间的转换规则为：

- 国标码 = 区位码 + 32
- 机内码 = 国标码 + 128 

讨论了这么多，读者能够发现，中文的编码比西文复杂多了。当年的计算机处理器都比较弱小，频繁的编码转换是一个沉重的负荷。于是这就催生了一门生意：专门研制一种处理汉字的东西：汉卡。联想、巨人、方正、金山，当年都是靠做汉卡起家的。

GB2312的缺点是编码空间不够大，不支持繁体中文和其它少数民族的文字。于是，另一种叫做GBK（汉字内码扩展规范）的兼容GB2312的编码方案被提出来了。它的做法比较简单，把国标码双字节的低字节，不再加128了，一下子就多出了很多码位，可以容纳3万多个编码，实际上收录了21000余个汉字，800多个符号，还有一些空余的位置。GBK并不是一个强制的国家标准，但是大多数操作系统里也都支持，成了汉字编码的事实标准。在中国台湾地区，计算机行业组织制定了一种称之为大五码（Big-5）作为繁体中文的编码规则，是繁体中文圈最常用的字符集。

早在1991年，一个名叫统一码联盟（Unicode Consortium）的NGO在美国山景城成立了，它的一些成员从1987年开始，就致力于开发一种包含世界上所有文字和符号的、通用的、标准化的字符集编码，叫做The Unicode Standard，以取代各种基于区域性语言的编码方案，1991年发布了1.0标准。ISO作为国际性的标准化组织，当然也很热衷于干这种大一统的事情，它于1993年发布了ISO/IEC 10646-1，即Unicode Character Set（UCS）。此后，UC和ISO也都认识到，没必要搞两套通用字符集，有悖于使命初心。于是两个组织开始协同合作，到Unicode 2.0标准发布时，它与UCS保持了同步，基本实现了兼容。但是，The Unicode Standard和UCS还是两套独立维护的标准，只不过Unicode的名声更响亮，影响更大。有些时候，权威的标准化组织发布的标准，并不一定会被遵循，而一些民间组织的方案往往会成为事实上的标准，被工业界广泛采用，XML、ECMAScript、Java等技术都将Unicode作为缺省的编码规范。

Unicode采用了一个类似于字典的表，将收录的每一个字符与一个编码对应上，每一个对应关系称之为Code Point（码位、码点），编码空间从0x00到0x10FFFF共1114111个Code Point。每2^16（65536，相当于一个unsigned short的范围）个Code Point被划分为成一个组，术语叫Plate，总共有17个Plate。Plate 0被称为Basic Multilingual Plane（BMP），范围是从0x00 - 0xFFFF，基本上容纳了大部分字符，其余16个Plate作为补充（4 - 13保留未分配、15 - 16可以用于私人扩展）。在BMP内部，按照字符特征划分成多个Block，其中比较重要的两块是ASCII和东亚象形文字：

- 0x00 - 0x7F（0 - 127）：这一个Block兼容了ASCII码，叫作Controls and Basic Latin，控制符和基本拉丁字符；
- 0x4E00 - 0x9FFC（19968 - 40956）：这一个Block盛放了中日韩的字符，叫作CJK Unified Ideographs，中日韩统一象形文字；

即时聊天习惯使用的表情符号，也被收录进了Unicdode，并且用单独划分了一个emoji的Block，Code Point范围是0x1F300 - 0X1F5FF(127744 - 128511)，位于第二个Plate（ Supplementary Multilingual Plate，SMP）之中。

Unicode中同样也有全角/半角的概念，它给每个字符都设置了一个East Asian Width（字符的东亚语言宽度）。全角字符的显示宽度是半角字符的两倍，所以字体的设计者、或者UI组件的开发者需要着重关注这个东西。在BMP范围内，可以简单的认为除了与ASCII码兼容的那一部分（0 - 127）之外都是全角字符。

我们日常使用到的大部分字符，基本都在BMP范围内（2个字节即可容纳），超出这个范围的则需要3个字节。在计算机的世界里，用多个字节作为一个整体来表示某个东西，就涉及字节顺序（Byte Order）问题，那就一并将字节顺序的知识讲一下。假设，我们需要将十进制数18，在内存/磁盘上存储起来，抑或通过网络将它发送给另一台计算机。存储和网络传输都是以字节为单位，并且按顺序进行的。18对应的4个字节，应该先处理左边第一个字节（0x0000），还是右边第一个字节（0x0010）呢？

是时候需要站队了。现在我们可以把世界上的人分成两派，一派喜欢从左往右（高字节、低地址），先存储 / 发送0x0000，称之为Big-Endian（大端，BE）：

```
[0x0000, 0x0000, 0x0001, 0x0010]
```

另一派人喜欢从右往左（低字节、低地址），先存储 / 发送0x0010，称之为Little-Endian（小端，LE）：

```
[0x0010, 0x0001, 0x000, 0x0000]
```

显然这两派人之间无法愉快地交流，大端派说我这个猪肉18块一斤，小端派一看你卖的是龙肉啊8448块一斤。

不知道为什么，连字符集编码这么复杂的事情都可以统一，而字节顺序这种二选一的简单问题，现实中却无法统一。采用大端或小端模式，并无明显的优劣之分，是由CPU体系架构决定的。有一些CPU比如SUN的SPARC（现在已经被Oracle废掉了）采用大端模式，X86采用小端模式，而ARM两种字节顺序都支持。操作系统采用哪种字节序，一般而言需要跟CPU保持一致，以避免没必要的转换开销。由于X86在桌面和服务器领域的垄断地位，ARM在移动端的垄断地位，所以Windows, Linux采用了小端模式，而Solaris和MacOS则采用了大端模式。TCP/IP协议使用了大端模式，因此，大端模式也叫作“网络字节序”。JAVA的虚拟机JVM采用了大端模式。一些标准的文件格式、或者一些跨平台的软件所产生的文件，都有自己的字节顺序，独立于底层软硬件架构，比如JPEG图像采用大端模式，Adobe的Photoshop也采用了大端模式，而另一种动态图像格式GIF则采用了小端模式。

回到Unicode的话题。我们把一段Unicode编码的内容存为磁盘文件，按照经验推断，这个文件里应该有一个标志，用来告诉应用软件，应该以何种字节顺序来解释这些内容。读者可以尝试用Windows自带的Notepad编辑一段文字存为UTF-8格式的文件（*.txt），然后用一个二进制浏览工具打开它，就会发现文件的前面多了三个字节：0xEE, 0xBB, 0xBF。这三个字节就叫做字节顺序标记。

## 2.2 集合 / 容器

## 2.3  文件和I/O

## 2.4 日期和时间

## 2.5 反射（Reflection）

